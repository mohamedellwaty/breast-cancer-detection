{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4iwwPyV18BT"
      },
      "outputs": [],
      "source": [ ,
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def remove_corrupted_images(dataset_root):\n",
        "    print(\"Checking for corrupted images...\")\n",
        "    removed = 0\n",
        "\n",
        "    if not os.path.exists(dataset_root):\n",
        "        raise FileNotFoundError(f\"Dataset root not found: {dataset_root}\")\n",
        "\n",
        "    for root, _, files in os.walk(dataset_root):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                continue\n",
        "            fp = os.path.join(root, f)\n",
        "            try:\n",
        "                Image.open(fp).verify()\n",
        "            except Exception:\n",
        "                try:\n",
        "                    os.remove(fp)\n",
        "                except:\n",
        "                    pass\n",
        "                removed += 1\n",
        "\n",
        "    print(f\"Removed corrupted images: {removed}\")\n",
        "\n",
        "\n",
        "def split_dataset(source_dir, dest_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    if os.path.exists(dest_dir):\n",
        "        # امسح القديم عشان مايتكرر نسخ\n",
        "        shutil.rmtree(dest_dir)\n",
        "\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    classes = [c for c in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, c))]\n",
        "    if not classes:\n",
        "        raise RuntimeError(\"No class folders found. لازم يكون عندك class0 و class1 جوه فولدر الداتا\")\n",
        "\n",
        "    for split_name in [\"train\", \"val\", \"test\"]:\n",
        "        for cls in classes:\n",
        "            os.makedirs(os.path.join(dest_dir, split_name, cls), exist_ok=True)\n",
        "\n",
        "    for cls in classes:\n",
        "        class_path = os.path.join(source_dir, cls)\n",
        "        images = [img for img in os.listdir(class_path) if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total = len(images)\n",
        "        if total == 0:\n",
        "            print(f\"Warning: class {cls} has 0 images\")\n",
        "            continue\n",
        "\n",
        "        train_end = int(train_ratio * total)\n",
        "        val_end = int((train_ratio + val_ratio) * total)\n",
        "\n",
        "        splits = {\n",
        "            \"train\": images[:train_end],\n",
        "            \"val\": images[train_end:val_end],\n",
        "            \"test\": images[val_end:]\n",
        "        }\n",
        "\n",
        "        for split_name, split_images in splits.items():\n",
        "            for img in split_images:\n",
        "                src = os.path.join(class_path, img)\n",
        "                dst = os.path.join(dest_dir, split_name, cls, img)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "    print(\"Dataset split completed\")\n",
        "\n",
        "\n",
        "IMG_SIZE = 50\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomGrayscale(p=0.25),\n",
        "    transforms.ColorJitter(brightness=0.35, contrast=0.35, saturation=0.25, hue=0.02),\n",
        "    transforms.RandomAutocontrast(p=0.25),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def make_loaders(split_root, batch_size=128, num_workers=4, pin_memory=True):\n",
        "    train_dir = os.path.join(split_root, \"train\")\n",
        "    val_dir = os.path.join(split_root, \"val\")\n",
        "    test_dir = os.path.join(split_root, \"test\")\n",
        "\n",
        "    train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    val_ds = datasets.ImageFolder(val_dir, transform=eval_transform)\n",
        "    test_ds = datasets.ImageFolder(test_dir, transform=eval_transform)\n",
        "\n",
        "    print(\"class_to_idx:\", train_ds.class_to_idx)\n",
        "\n",
        "    # Balance sampler عشان class0 عندك أكبر بكتير\n",
        "    targets = [y for _, y in train_ds.samples]\n",
        "    class_counts = torch.bincount(torch.tensor(targets))\n",
        "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
        "    sample_weights = class_weights[torch.tensor(targets)]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, sampler=sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, train_ds\n",
        "\n",
        "\n",
        "class CNN_Manual_50(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 50 -> 25\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 25 -> 12\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 12 -> 6\n",
        "\n",
        "            nn.Dropout(0.25),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 6 * 6, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_sum = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "        pred = out.argmax(dim=1)\n",
        "        total += y.size(0)\n",
        "        correct += (pred == y).sum().item()\n",
        "\n",
        "    acc = 100.0 * correct / max(1, total)\n",
        "    avg_loss = loss_sum / max(1, len(loader))\n",
        "    return acc, avg_loss\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, device, epochs=10, lr=1e-3, save_path=\"cnn_manual_50.pth\"):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
        "\n",
        "    best_val_acc = -1.0\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "\n",
        "        loss_sum = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            pred = out.argmax(dim=1)\n",
        "            total += y.size(0)\n",
        "            correct += (pred == y).sum().item()\n",
        "\n",
        "        train_acc = 100.0 * correct / max(1, total)\n",
        "        train_loss = loss_sum / max(1, len(train_loader))\n",
        "\n",
        "        val_acc, val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        print(f\"Epoch {ep}/{epochs} | Train Loss {train_loss:.4f} Acc {train_acc:.2f}% | Val Loss {val_loss:.4f} Acc {val_acc:.2f}% | Time {dt:.1f}s\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"class_to_idx\": getattr(train_loader.dataset, \"class_to_idx\", None),\n",
        "                \"img_size\": IMG_SIZE,\n",
        "                \"arch\": \"CNN_Manual_50\"\n",
        "            }, save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    RAW_PATH = r\"D:\\mohamed\\deeplearning\\project\\project\\ImageFolder1\"\n",
        "    SPLIT_PATH = r\"D:\\mohamed\\deeplearning\\project\\project\\ImageFolder101\"\n",
        "    MODEL_PATH = r\"D:\\mohamed\\deeplearning\\project\\project\\cnn_manual_51.pth\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    print(\"Torch version:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "    remove_corrupted_images(RAW_PATH)\n",
        "    split_dataset(RAW_PATH, SPLIT_PATH)\n",
        "\n",
        "    train_loader, val_loader, test_loader, train_ds = make_loaders(\n",
        "        SPLIT_PATH, batch_size=128, num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    num_classes = len(train_ds.class_to_idx)\n",
        "    model = CNN_Manual_50(num_classes=num_classes).to(device)\n",
        "\n",
        "    train(model, train_loader, val_loader, device, epochs=10, lr=1e-3, save_path=MODEL_PATH)\n",
        "\n",
        "    test_acc, test_loss = evaluate(model, test_loader, nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}% | Test Loss: {test_loss:.4f}\")\n",
        "    print(\"Final model path:\", MODEL_PATH)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
