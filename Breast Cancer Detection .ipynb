{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T05:24:19.048721Z",
     "iopub.status.busy": "2025-12-27T05:24:19.048158Z",
     "iopub.status.idle": "2025-12-27T06:14:50.437959Z",
     "shell.execute_reply": "2025-12-27T06:14:50.436853Z",
     "shell.execute_reply.started": "2025-12-27T05:24:19.048693Z"
    },
    "id": "N4iwwPyV18BT",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Torch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Checking for corrupted images...\n",
      "Removed corrupted images: 0\n",
      "Dataset split completed\n",
      "class_to_idx: {'class0': 0, 'class1': 1}\n",
      "Epoch 1/10 | Train Loss 0.5097 Acc 78.02% | Val Loss 0.4349 Acc 80.40% | Time 123.1s\n",
      "Model saved to /kaggle/working/cnn_manual_50.pth\n",
      "Epoch 2/10 | Train Loss 0.4572 Acc 79.86% | Val Loss 0.4347 Acc 80.20% | Time 119.8s\n",
      "Epoch 3/10 | Train Loss 0.4439 Acc 80.74% | Val Loss 0.3450 Acc 85.60% | Time 119.9s\n",
      "Model saved to /kaggle/working/cnn_manual_50.pth\n",
      "Epoch 4/10 | Train Loss 0.4245 Acc 81.67% | Val Loss 0.4336 Acc 81.68% | Time 119.8s\n",
      "Epoch 5/10 | Train Loss 0.4118 Acc 82.31% | Val Loss 0.3483 Acc 85.47% | Time 118.5s\n",
      "Epoch 6/10 | Train Loss 0.3984 Acc 82.99% | Val Loss 0.4525 Acc 78.75% | Time 118.0s\n",
      "Epoch 7/10 | Train Loss 0.3938 Acc 83.33% | Val Loss 0.3511 Acc 83.92% | Time 117.5s\n",
      "Epoch 8/10 | Train Loss 0.3832 Acc 83.83% | Val Loss 0.3324 Acc 85.52% | Time 117.9s\n",
      "Epoch 9/10 | Train Loss 0.3749 Acc 84.24% | Val Loss 0.2732 Acc 87.98% | Time 118.4s\n",
      "Model saved to /kaggle/working/cnn_manual_50.pth\n",
      "Epoch 10/10 | Train Loss 0.3711 Acc 84.45% | Val Loss 0.2926 Acc 87.35% | Time 119.0s\n",
      "Test Accuracy: 87.45% | Test Loss: 0.2898\n",
      "Final model path: /kaggle/working/cnn_manual_50.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def remove_corrupted_images(dataset_root):\n",
    "    print(\"Checking for corrupted images...\")\n",
    "    removed = 0\n",
    "\n",
    "    if not os.path.exists(dataset_root):\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {dataset_root}\")\n",
    "\n",
    "    for root, _, files in os.walk(dataset_root):\n",
    "        for f in files:\n",
    "            if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            fp = os.path.join(root, f)\n",
    "            try:\n",
    "                Image.open(fp).verify()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    os.remove(fp)\n",
    "                except:\n",
    "                    pass\n",
    "                removed += 1\n",
    "\n",
    "    print(f\"Removed corrupted images: {removed}\")\n",
    "\n",
    "\n",
    "def split_dataset(source_dir, dest_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    if os.path.exists(dest_dir):\n",
    "        # امسح القديم عشان مايتكرر نسخ\n",
    "        shutil.rmtree(dest_dir)\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    classes = [c for c in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, c))]\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"No class folders found. لازم يكون عندك class0 و class1 جوه فولدر الداتا\")\n",
    "\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            os.makedirs(os.path.join(dest_dir, split_name, cls), exist_ok=True)\n",
    "\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(source_dir, cls)\n",
    "        images = [img for img in os.listdir(class_path) if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        total = len(images)\n",
    "        if total == 0:\n",
    "            print(f\"Warning: class {cls} has 0 images\")\n",
    "            continue\n",
    "\n",
    "        train_end = int(train_ratio * total)\n",
    "        val_end = int((train_ratio + val_ratio) * total)\n",
    "\n",
    "        splits = {\n",
    "            \"train\": images[:train_end],\n",
    "            \"val\": images[train_end:val_end],\n",
    "            \"test\": images[val_end:]\n",
    "        }\n",
    "\n",
    "        for split_name, split_images in splits.items():\n",
    "            for img in split_images:\n",
    "                src = os.path.join(class_path, img)\n",
    "                dst = os.path.join(dest_dir, split_name, cls, img)\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "    print(\"Dataset split completed\")\n",
    "\n",
    "\n",
    "IMG_SIZE = 50\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomGrayscale(p=0.25),\n",
    "    transforms.ColorJitter(brightness=0.35, contrast=0.35, saturation=0.25, hue=0.02),\n",
    "    transforms.RandomAutocontrast(p=0.25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def make_loaders(split_root, batch_size=128, num_workers=4, pin_memory=True):\n",
    "    train_dir = os.path.join(split_root, \"train\")\n",
    "    val_dir = os.path.join(split_root, \"val\")\n",
    "    test_dir = os.path.join(split_root, \"test\")\n",
    "\n",
    "    train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    val_ds = datasets.ImageFolder(val_dir, transform=eval_transform)\n",
    "    test_ds = datasets.ImageFolder(test_dir, transform=eval_transform)\n",
    "\n",
    "    print(\"class_to_idx:\", train_ds.class_to_idx)\n",
    "\n",
    "    # Balance sampler عشان class0 عندك أكبر بكتير\n",
    "    targets = [y for _, y in train_ds.samples]\n",
    "    class_counts = torch.bincount(torch.tensor(targets))\n",
    "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "    sample_weights = class_weights[torch.tensor(targets)]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, sampler=sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds\n",
    "\n",
    "\n",
    "class CNN_Manual_50(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 50 -> 25\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 25 -> 12\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 12 -> 6\n",
    "\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "    acc = 100.0 * correct / max(1, total)\n",
    "    avg_loss = loss_sum / max(1, len(loader))\n",
    "    return acc, avg_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, device, epochs=10, lr=1e-3, save_path=\"cnn_manual_50.pth\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            pred = out.argmax(dim=1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "\n",
    "        train_acc = 100.0 * correct / max(1, total)\n",
    "        train_loss = loss_sum / max(1, len(train_loader))\n",
    "\n",
    "        val_acc, val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        print(f\"Epoch {ep}/{epochs} | Train Loss {train_loss:.4f} Acc {train_acc:.2f}% | Val Loss {val_loss:.4f} Acc {val_acc:.2f}% | Time {dt:.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"class_to_idx\": getattr(train_loader.dataset, \"class_to_idx\", None),\n",
    "                \"img_size\": IMG_SIZE,\n",
    "                \"arch\": \"CNN_Manual_50\"\n",
    "            }, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    RAW_PATH = \"/kaggle/input/imagefolder2/Dataset\"\n",
    "    SPLIT_PATH = \"/kaggle/working/cleaned_data\"\n",
    "    MODEL_PATH = r\"/kaggle/working/cnn_manual_50.pth\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "    remove_corrupted_images(RAW_PATH)\n",
    "    split_dataset(RAW_PATH, SPLIT_PATH)\n",
    "\n",
    "    train_loader, val_loader, test_loader, train_ds = make_loaders(\n",
    "        SPLIT_PATH, batch_size=128, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    num_classes = len(train_ds.class_to_idx)\n",
    "    model = CNN_Manual_50(num_classes=num_classes).to(device)\n",
    "\n",
    "    train(model, train_loader, val_loader, device, epochs=10, lr=1e-3, save_path=MODEL_PATH)\n",
    "\n",
    "    test_acc, test_loss = evaluate(model, test_loader, nn.CrossEntropyLoss(), device)\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "    print(\"Final model path:\", MODEL_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T06:15:51.128587Z",
     "iopub.status.busy": "2025-12-27T06:15:51.128063Z",
     "iopub.status.idle": "2025-12-27T06:16:43.939430Z",
     "shell.execute_reply": "2025-12-27T06:16:43.938694Z",
     "shell.execute_reply.started": "2025-12-27T06:15:51.128551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Contents: ['train', 'val', 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/cleaned_data.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "cleaned_dir = \"/kaggle/working/cleaned_data\"\n",
    "\n",
    "print(\"Exists:\", os.path.exists(cleaned_dir))\n",
    "print(\"Contents:\", os.listdir(cleaned_dir))\n",
    "\n",
    "shutil.make_archive(\n",
    "    \"/kaggle/working/cleaned_data\",  # output zip name\n",
    "    \"zip\",\n",
    "    cleaned_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8933504,
     "sourceId": 14028996,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
